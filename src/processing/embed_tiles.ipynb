{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import faiss\n",
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model      = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor  = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tiles_dir  = \"../../data/tiles/\"\n",
    "tile_files = [os.path.join(tiles_dir, fname)\n",
    "              for fname in os.listdir(tiles_dir) if fname.lower().endswith(\".png\")]\n",
    "\n",
    "embeddings_list = []\n",
    "tile_mapping    = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tile_path in tile_files: \n",
    "    image        = Image.open(tile_path).convert(\"RGB\")\n",
    "    inputs       = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        image_features = model.get_image_features(pixel_values)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    embeddings_list.append(image_features.cpu().numpy())\n",
    "    tile_mapping.append(tile_path)\n",
    "\n",
    "tile_embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "d = tile_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "loss_fn      = lpips.LPIPS(net='alex')\n",
    "transform    = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "image_dir    = \"../../data/tiles/\"\n",
    "image_names  = []\n",
    "feature_list = []\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "\n",
    "    if filename.endswith((\".jpg\", \".png\")):\n",
    "\n",
    "        img        = Image.open(os.path.join(image_dir, filename)).convert(\"RGB\")\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = loss_fn.net(img_tensor)\n",
    "            feature_vector = torch.cat([f.mean(dim=[2, 3]).flatten() for f in features], dim=0)\n",
    "            feature_vector = feature_vector.cpu().numpy().flatten()\n",
    "        image_names.append(filename)\n",
    "        feature_list.append(feature_vector)\n",
    "\n",
    "feature_matrix = np.array(feature_list, dtype=np.float32)\n",
    "index = faiss.IndexFlatL2(feature_matrix.shape[1]) \n",
    "index.add(feature_matrix)\n",
    "\n",
    "faiss.write_index(index, \"../../data/output/lpips_index.faiss\")\n",
    "np.save(\"../../data/output/image_names.npy\", image_names)\n",
    "print(\"LPIPS features saved and FAISS index built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),          \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "best_lpips_score    = 0\n",
    "ref                 = Image.open('../../data/examples/maps/house_map.jpg').convert('RGB')\n",
    "img1_tensor         = transform(ref).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    features       = loss_fn.net(img1_tensor) \n",
    "    feature_vector = torch.cat([f.mean(dim=[2, 3]).flatten() for f in features], dim=0)\n",
    "    feature_vector = feature_vector.cpu().numpy().flatten()\n",
    "\n",
    "D, I             = index.search(np.array([feature_vector], dtype=np.float32), 10)\n",
    "best_lpips_score = 0\n",
    "\n",
    "for i in range(10):\n",
    "    tile_path   = image_names[I[0][i]]\n",
    "    tile        = Image.open(os.path.join(image_dir, tile_path)).convert(\"RGB\")\n",
    "    tile_tensor = transform(tile).unsqueeze(0)\n",
    "    lpips_score = loss_fn(tile_tensor, img1_tensor).item()\n",
    "    \n",
    "    print(f\"Tile: {tile_path}, LPIPS: {lpips_score}\")\n",
    "    if lpips_score > best_lpips_score:\n",
    "        best_lpips_score = lpips_score\n",
    "        best_tile_path = tile_path\n",
    "\n",
    "tile      = Image.open(os.path.join(image_dir, best_tile_path)).convert(\"RGB\")\n",
    "tile.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(tile_embeddings)\n",
    "faiss.write_index(index, '../../data/output/my_tiles_file.index')\n",
    "with open('../../data/output/tile_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(tile_mapping, f)\n",
    "\n",
    "print(f\"Indexed {index.ntotal} tiles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_closest_tile(query_image_path, k=1):\n",
    "    \"\"\"\n",
    "    Given a query image, compute its CLIP embedding and retrieve the closest tile from the FAISS index.\n",
    "    \"\"\"\n",
    "    query_image  = Image.open(query_image_path).convert(\"RGB\")\n",
    "    inputs       = processor(images=query_image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_features = model.get_image_features(pixel_values)\n",
    "        query_features = query_features / query_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    query_features           = query_features.cpu().numpy()\n",
    "    distances, indices       = index.search(query_features, k)\n",
    "\n",
    "    closest_tile_path = tile_mapping[indices[0][0]]\n",
    "    return closest_tile_path, distances[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_closest_label(query_image_path, text_labels, k=1):\n",
    "    \n",
    "    query_image  = Image.open(query_image_path).convert(\"RGB\")\n",
    "    inputs       = processor(images=query_image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_features = model.get_image_features(pixel_values)\n",
    "        query_features = query_features / query_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    query_features = query_features.cpu().numpy()\n",
    "    inputs_text    = processor(text=text_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad(): \n",
    "        text_features   = model.get_text_features(**inputs_text)\n",
    "        text_features   = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    text_embeddings = text_features.cpu().numpy()\n",
    "    similarities    = (query_features @ text_embeddings.T)[0]\n",
    "    \n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "    top_k_labels  = [text_labels[i] for i in top_k_indices]\n",
    "    top_k_scores  = [similarities[i] for i in top_k_indices]\n",
    "    \n",
    "    if k == 1:\n",
    "        return top_k_labels[0], top_k_scores[0]\n",
    "    else:\n",
    "        return top_k_labels, top_k_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path             = \"../data/examples/structures/bridge.jpg\"\n",
    "text_labels            = [\"house\", \"rock\", \"forest\", \"dirt\", \"grass\", \"water\"]\n",
    "closest_tile, distance = retrieve_closest_tile(query_path)\n",
    "closest_label          = retrieve_closest_label(query_path, text_labels)\n",
    "print(f\"Closest label: {closest_label} (distance: {distance:.4f})\")\n",
    "\n",
    "tile_image = Image.open(closest_tile)\n",
    "tile_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
